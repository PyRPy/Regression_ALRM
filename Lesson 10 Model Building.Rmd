---
title: "Lesson 10: Model Building"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

https://onlinecourses.science.psu.edu/stat501/node/428/

# Martians (underspecified model)
Load the martians data.
Fit a multiple linear regression model of weight vs height + water.
```{r}
martian <- read.csv("DataL10/martian.csv", header=T)
head(martian)
attach(martian)
pairs(martian, lower.panel = NULL)
model.1 <- lm(weight ~ height + water)
summary(model.1)
```
Fit a simple linear regression model of weight vs height.
```{r}
model.2 <- lm(weight ~ height)
summary(model.2)
```
Create a scatterplot of weight vs height with points marked by water level and regression lines for each model.
```{r}
plot(x=height, y=weight, col=water/10+1,
     panel.last = c(lines(sort(height[water==0]),
                          fitted(model.1)[water==0][order(height[water==0])],
                          col=1),
                    lines(sort(height[water==10]),
                          fitted(model.1)[water==10][order(height[water==10])],
                          col=2),
                    lines(sort(height[water==20]),
                          fitted(model.1)[water==20][order(height[water==20])],
                          col=3),
                    lines(sort(height), fitted(model.2)[order(height)], col=4)))
legend("topleft", col=1:4, pch=1, lty=1, inset=0.02,
       legend=c("Model 1, water=0", "Model 1, water=10",
                "Model 1, water=20", "Model 2"))

detach(martian)
```
* comments from lesson note
As you can see, if we use the blue line to predict the weight of a randomly selected martian, we would consistently overestimate the weight of martians who drink 0 cups of water a day, and we would consistently understimate the weight of martians who drink 20 cups of water a day. That is, our predicted responses would be biased.

# Cement hardening (variable selection using stepwise regression)

* Response y: heat evolved in calories during hardening of cement on a per gram basis
* Predictor x1: % of tricalcium aluminate
* Predictor x2: % of tricalcium silicate
* Predictor x3: % of tetracalcium alumino ferrite
* Predictor x4: % of dicalcium silicate

Load the cement data.
Create a scatterplot matrix of the data.
```{r message=FALSE}
cement <- read.table("DataL10/cement.txt", header=T)
attach(cement)

pairs(cement, lower.panel = NULL)
```
Use the add1 and drop1 functions to conduct stepwise regression.
```{r}
model.0 <- lm(y ~ 1)
add1(model.0, ~ x1 + x2 + x3 + x4, test="F")
```
from step above x4 is selected after considering AIC 58.852
```{r}
model.4 <- lm(y ~ x4)
add1(model.4, ~ . + x1 + x2 + x3, test="F")
```
to add x1
```{r}
model.14 <- lm(y ~ x1 + x4)
drop1(model.14, ~ ., test="F")
```
don't drop x1 or x4
```{r}
add1(model.14, ~ . + x2 + x3, test="F")
```
to add x2 instead of x3
```{r}
model.124 <- lm(y ~ x1 + x2 + x4)
drop1(model.124, ~ ., test="F")
```
you can drop x4 (not significant change after drop x4)
finally select x1 and x2 as predictors
```{r}
model.12 <- lm(y ~ x1 + x2)
add1(model.12, ~ . + x3 + x4, test="F") # also has higg p-value
detach(cement)
```
# IQ and body size (variable selection using stepwise regression)

* Response (y): Performance IQ scores (PIQ) from the revised Wechsler Adult Intelligence Scale. This variable served as the investigator's measure of the individual's intelligence.
* Potential predictor (x1): Brain size based on the count obtained from MRI scans (given as count/10,000).
* Potential predictor (x2): Height in inches.
* Potential predictor (x3): Weight in pounds.

Load the iqsize data.
Create a scatterplot matrix of the data.
```{r message=FALSE}
iqsize <- read.table("DataL10/iqsize.txt", header=T)
attach(iqsize)

pairs(iqsize, lower.panel = NULL)
```
Use the add1 and drop1 functions to conduct stepwise regression.
```{r}
model.0 <- lm(PIQ ~ 1)
add1(model.0, ~ Brain + Height + Weight, test="F")
```
brain as predictor has lowest AIC and p-value
```{r}
model.1 <- lm(PIQ ~ Brain)
add1(model.1, ~ . + Height + Weight, test="F")
```
to add the 'Height' to the equation
```{r}
model.12 <- lm(PIQ ~ Brain + Height)
drop1(model.12, ~ ., test="F")
```
don't drop any, they are all 'significant'
```{r}
add1(model.12, ~ . + Weight, test="F")
detach(iqsize)
```
from p-value, we can tell 'Weight' is insignificant

# Blood pressure (variable selection using stepwise regression)

* blood pressure (y = BP, in mm Hg)
* age (x1 = Age, in years)
* weight (x2 = Weight, in kg)
* body surface area (x3 = BSA, in sq m)
* duration of hypertension (x4 = Dur, in years)
* basal pulse (x5 = Pulse, in beats per minute)
* stress index (x6 = Stress)

Load the bloodpress data.
Create scatterplot matrices of the data.
```{r message=FALSE}
bloodpress <- read.table("DataL10/bloodpress.txt", header=T)
attach(bloodpress)

pairs(bloodpress[,c(2:5)])
```
```{r}
pairs(bloodpress[,c(2,6:8)])
```
Use the add1 and drop1 functions to conduct stepwise regression.
```{r}
model.0 <- lm(BP  ~ 1)
add1(model.0, ~ Age + Weight + BSA + Dur + Pulse + Stress, test="F")
```
weight has the lowest AIC, to use 'weight' first
```{r}
model.2 <- lm(BP ~ Weight)
add1(model.2, ~ . + Age + BSA + Dur + Pulse + Stress, test="F")
```
to add 'Age' as second predictor
```{r}
model.12 <- lm(BP ~ Age + Weight)
drop1(model.12, ~ ., test="F")
```
don't any , Age and Weight are all significant
```{r}
add1(model.12, ~ . + BSA + Dur + Pulse + Stress, test="F")
```
to add BSA
```{r}
model.123 <- lm(BP ~ Age + Weight + BSA)
drop1(model.123, ~ ., test="F")
```
to keep all three predictors
```{r}
add1(model.123, ~ . + Dur + Pulse + Stress, test="F")
detach(bloodpress)
```
p-value above from "F" test, shows there is no significant predictors anymore

# Cement hardening (variable selection using best subsets regression)

Load the cement data.
```{r}
cement <- read.table("DataL10/cement.txt", header=T)
attach(cement)
```

Use the regsubsets function in the leaps package to conduct variable selection using exhaustive search (i.e., best subsets regression). Note that the nbest=2 argument returns the best two models with 1, 2, ..., k predictors.
```{r message=FALSE}
library(leaps)
subset <- regsubsets(y ~ x1 + x2 + x3 + x4, method="exhaustive", nbest=2, data=cement)
cbind(summary(subset)$outmat, round(summary(subset)$adjr2, 3), round(summary(subset)$cp, 1))
```
Fit models with all four predictors (assumed unbiased) and just two predictors to retrieve the information needed to calculate Cp for the model with just two predictors by hand.
```{r}
model.1234 <- lm(y ~ x1 + x2 + x3 + x4)
model.12 <- lm(y ~ x1 + x2)
```
```{r}
SSE.k <- sum(residuals(model.12)^2) # SSE_k = 57.90448
MSE.all <- summary(model.1234)$sigma^2 # MSE_all = 5.982955
params <- summary(model.12)$df[1] # k+1 = 3
n <- sum(summary(model.1234)$df[1:2]) # n = 13
SSE.k/MSE.all + 2*params - n # Cp = 2.678242
```
```{r}
model.14 <- lm(y ~ x1 + x4)
SSE.k <- sum(residuals(model.14)^2) # SSE_k = 74.76211
params <- summary(model.14)$df[1] # k+1 = 3
SSE.k/MSE.all + 2*params - n # Cp = 5.495851
```
Fit model with x1, x2, and x4 and note the variance inflation factors for x2 and x4 are very high.
```{r}
model.124 <- lm(y ~ x1 + x2 + x4)
library(car)
vif(model.124)
```
Fit model with x1, x2, and x3 and note the variance inflation factors are acceptable.
```{r}
model.123 <- lm(y ~ x1 + x2 + x3)
vif(model.123)
```
Fit model with x1 and x2 and note the variance inflation factors are acceptable, adjusted R2 is high, and a residual analysis and normality test yields no concerns.
```{r}
summary(model.12)
```
```{r}
vif(model.12)
```
```{r}
plot(x=fitted(model.12), y=rstandard(model.12),
     panel.last = abline(h=0, lty=2))

```
```{r}
qqnorm(rstandard(model.12), main="", datax=TRUE)
qqline(rstandard(model.12), datax=TRUE)
```
```{r}
library(nortest)
ad.test(rstandard(model.12)) # A = 0.6136, p-value = 0.08628

detach(cement)
```
IQ and body size (variable selection using best subsets regression)

    Load the iqsize data.
    Use the regsubsets function in the leaps package to conduct variable selection using exhaustive search (i.e., best subsets regression).
    Fit model with Brain and Height and note the variance inflation factors are acceptable, adjusted R2 is as good as it gets with this dataset, and a residual analysis and normality test yields no concerns.
```{r}
iqsize <- read.table("DataL10/iqsize.txt", header=T)
attach(iqsize)

subset <- regsubsets(PIQ ~ Brain + Height + Weight, method="exhaustive", nbest=2, data=iqsize)
cbind(summary(subset)$outmat, round(summary(subset)$adjr2, 3), round(summary(subset)$cp, 1))
```
```{r}
model.12 <- lm(PIQ ~ Brain + Height)
summary(model.12)
```
```{r}
vif(model.12)
```
```{r}

par(mfrow=c(1,2))

plot(x=fitted(model.12), y=rstandard(model.12),
     panel.last = abline(h=0, lty=2))

qqnorm(rstandard(model.12), main="", datax=TRUE)
qqline(rstandard(model.12), datax=TRUE)

```
```{r}
ad.test(rstandard(model.12)) # A = 0.2629, p-value = 0.6829

detach(iqsize)
```
Blood pressure (variable selection using best subsets regression)

    Load the bloodpress data.
    Use the regsubsets function in the leaps package to conduct variable selection using exhaustive search (i.e., best subsets regression).
    Fit model with Age and Weight and note the variance inflation factors are acceptable, adjusted R2 can't get much better, and a residual analysis and normality test yields no concerns.
```{r message=FALSE}
bloodpress <- read.table("DataL10/bloodpress.txt", header=T)
attach(bloodpress)

subset <- regsubsets(BP ~ Age + Weight + BSA + Dur + Pulse + Stress,
                     method="exhaustive", nbest=2, data=bloodpress)
cbind(summary(subset)$outmat, round(summary(subset)$adjr2, 3),
      round(summary(subset)$cp, 1))
```
```{r}
model.12 <- lm(BP ~ Age + Weight)
summary(model.12)
```
```{r}
vif(model.12)
```
```{r}
par(mfrow=c(1,2))
plot(x=fitted(model.12), y=rstandard(model.12),
     panel.last = abline(h=0, lty=2))

qqnorm(rstandard(model.12), main="", datax=TRUE)
qqline(rstandard(model.12), datax=TRUE)
```
```{r}
ad.test(rstandard(model.12)) # A = 0.275, p-value = 0.6225

detach(bloodpress)
```
Peruvian blood pressure (variable selection using best subsets regression)

    Load the peru data.
    Use the regsubsets function in the leaps package to conduct variable selection using exhaustive search (i.e., best subsets regression).
    Fit the best 5-predictor and 4-predictor models.
    Calculate AIC and BIC by hand.
    Use the stepAIC function in the MASS package to conduct variable selection using a stepwise algorithm based on AIC or BIC.
```{r message=FALSE}
peru <- read.table("DataL10/peru.txt", header=T)
attach(peru)

fraclife <- Years/Age

n <- length(Systol) # 39

subset <- regsubsets(Systol ~ Age + Years + fraclife + Weight + Height + Chin +
                       Forearm + Pulse,
                     method="exhaustive", nbest=2, data=peru)
cbind(summary(subset)$outmat, round(summary(subset)$rsq, 3),
      round(summary(subset)$adjr2, 3), round(summary(subset)$cp, 1),
      round(sqrt(summary(subset)$rss/(n-c(rep(1:7,rep(2,7)),8)-1)), 4))
#          Age Years fraclife Weight Height Chin Forearm Pulse         
```
```{r}
model.5 <- lm(Systol ~ Age + Years + fraclife + Weight + Chin)
summary(model.5)
```
```{r}
k <- 5
n*log(sum(residuals(model.5)^2))-n*log(n)+2*(k+1) # AIC = 172.0151
n*log(sum(residuals(model.5)^2))-n*log(n)+log(n)*(k+1) # BIC = 181.9965

```
```{r}
model.4 <- lm(Systol ~ Age + Years + fraclife + Weight)
summary(model.4)
```
```{r}
k <- 4
n*log(sum(residuals(model.4)^2))-n*log(n)+2*(k+1) # AIC = 174.2316
n*log(sum(residuals(model.4)^2))-n*log(n)+log(n)*(k+1) # BIC = 182.5494

```
```{r}
library(MASS)
subset.aic <- stepAIC(lm(Systol ~ Age + Years + fraclife + Weight + Height +
                           Chin + Forearm + Pulse), direction="both", k=2)
```
```{r}
subset.bic <- stepAIC(lm(Systol ~ Age + Years + fraclife + Weight + Height +
                           Chin + Forearm + Pulse), direction="both", k=log(n))
detach(peru)
```
Measurements of college students (variable selection using stepwise regression)

    Load the Physical data.
    Use the add1 and drop1 functions to conduct stepwise regression.
    Use the regsubsets function to conduct variable selection using backward elimination.
    Use the regsubsets function to conduct variable selection using forward selection.
```{r message=FALSE}
physical <- read.table("DataL10/Physical.txt", header=T)
attach(physical)

gender <- ifelse(Sex=="Female",1,0)

model.0 <- lm(Height ~ 1)
add1(model.0, ~ LeftArm + LeftFoot + LeftHand + HeadCirc + nose + gender, test="F")
```
```{r}
model.2 <- lm(Height ~ LeftFoot)
add1(model.2, ~ . + LeftArm + LeftHand + HeadCirc + nose + gender, test="F")
```
```{r}
model.12 <- lm(Height ~ LeftArm + LeftFoot)
drop1(model.12, ~ ., test="F")
```
```{r}
add1(model.12, ~ . + LeftHand + HeadCirc + nose + gender, test="F")
```
```{r}
subset <- regsubsets(Height ~ LeftArm + LeftFoot + LeftHand + HeadCirc + nose + gender,
                     method="backward", data=physical)

```
```{r}
subset <- regsubsets(Height ~ LeftArm + LeftFoot + LeftHand + HeadCirc + nose + gender,
                     method="forward", data=physical)

detach(physical)
```

